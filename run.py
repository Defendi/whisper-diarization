import whisper
import torch
import os
from pyannote.audio import Pipeline

AUDIO_FILE = "audio/Primeira_reuni√£o_27-08-2025 18.48.mp3"
OUTPUT_FILE = "output/Primeira_reuni√£o_com_falantes.txt"

HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN")
if not HUGGINGFACE_TOKEN:
    raise ValueError("‚ùå Token do Hugging Face n√£o encontrado. Defina HUGGINGFACE_TOKEN.")

# Vrifica√ß√£o de GPU
device = "cuda" if torch.cuda.is_available() else "cpu"

# Transcri√ß√£o com Whisper
print(f"üî§ Transcrevendo com Whisper no {device}...")
model = whisper.load_model("medium", device=device)
result = model.transcribe(AUDIO_FILE,
                          task="transcribe",
                          fp16=(device=="cuda"),
                          language="pt",
                          temperature=0.0)

# Diariza√ß√£o com PyAnnote
print(f"üîä Realizando diariza√ß√£o com PyAnnote... ({HUGGINGFACE_TOKEN})")
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization@2.1", use_auth_token=HUGGINGFACE_TOKEN)

# For√ßa uso de GPU
if device == "cuda":
    print("üöÄ Usando GPU para diariza√ß√£o")
    pipeline.to(torch.device("cuda"))

# Realizando diariza√ß√£o
print("üé§ Processando √°udio para diariza√ß√£o...")
diarization = pipeline(AUDIO_FILE)

# Combina√ß√£o dos dados
print("üß† Combinando falas com falantes...")
def get_speaker(time, diarization):
    for segment, _, speaker in diarization.itertracks(yield_label=True):
        if segment.start <= time <= segment.end:
            return speaker
    return "Desconhecido"

final_text = ""
for segment in result["segments"]:
    speaker = get_speaker(segment["start"], diarization)
    text = segment["text"].strip()
    final_text += f"[{speaker}] {text}\n"

# Salvando sa√≠da
os.makedirs("output", exist_ok=True)
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    f.write(final_text)

print(f"‚úÖ Transcri√ß√£o com falantes salva em: {OUTPUT_FILE}")
